{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/joseortegalabra/MLBI/blob/master/Tarea2_MLBI_Jos%C3%A9_Ignacio_Ortega_Labra.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ySBA5Y7QlPFQ"
   },
   "source": [
    "# Tarea 2: Machine Learning for Business Intelligence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c4N_6UJ4lPFS"
   },
   "source": [
    "## Instrucciones\n",
    "\n",
    "1. Esta tarea es de caracter individual, por lo tanto queda estrictamente prohibida la copia de codigo ajeno.\n",
    "\n",
    "2. En caso de elaborar su respuesta **basandose** en código ajeno encontrado en la web, debe agradecer al menos citando la referencia.\n",
    "\n",
    "3. La discusión con compañeros es siempre bienvenida, no obstante cada respuesta debe ser redactada de manera individual.\n",
    "\n",
    "4. La fecha límite para la entrega de esta tarea queda establecida para el 15/06/2020.\n",
    "\n",
    "Nota: La resolución de algunos de los ejercicios puede tomar bastante tiempo, así como su ejecución. Por lo tanto se recomienda hacer la tarea con anticipación y evitar riesgos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Rcq7p_obH9o5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow.keras as kr\n",
    "from sklearn.metrics import accuracy_score\n",
    "import math\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sgZ8LXbyba7F"
   },
   "outputs": [],
   "source": [
    "#funciones auxiliares usadas para graficar\n",
    "def visualize_train_valid_test(x_train, x_valid, x_test, y_train, y_valid, y_test):\n",
    "  fig, axs = plt.subplots(3, 1, figsize = ((7,20)) )\n",
    "  #plot train\n",
    "  axs[0].scatter(x_train[:, 0], x_train[:, 1], c = y_train, alpha = 0.5, marker = 'x')\n",
    "  axs[0].set_title('Data Train', fontsize = 25)\n",
    "  axs[0].set_yticklabels([])\n",
    "  axs[0].set_xticklabels([])\n",
    "\n",
    "  #plot valid\n",
    "  axs[1].scatter(x_valid[:, 0], x_valid[:, 1], c = y_valid, alpha = 0.5, marker = 'x')\n",
    "  axs[1].set_title('Data Valid', fontsize = 25)\n",
    "  axs[1].set_yticklabels([])\n",
    "  axs[1].set_xticklabels([])\n",
    "\n",
    "  #plot test\n",
    "  axs[2].scatter(x_test[:, 0], x_test[:, 1], c = y_test, alpha = 0.5, marker = 'x')\n",
    "  axs[2].set_title('Data Test', fontsize = 25)\n",
    "  axs[2].set_yticklabels([])\n",
    "  axs[2].set_xticklabels([])\n",
    "\n",
    "def visualize_results(train_loss, val_loss, train_acc, val_acc):\n",
    "  fig, axs = plt.subplots(nrows = 1, ncols = 2, figsize = ((14, 7)))\n",
    "  ax = axs[0]\n",
    "  ax.set_title('Loss', fontsize = 25)\n",
    "  ax.grid(color = 'black', alpha = 0.5, linestyle = 'dashed', linewidth = 0.5)\n",
    "  ax.plot(train_loss, label = 'loss_train', color = 'black', linestyle = '-')\n",
    "  ax.plot(val_loss, label = 'loss_valid', color = 'orange')\n",
    "  ax.set_xlabel('Epochs', fontsize = 13)\n",
    "  ax.legend()\n",
    "\n",
    "  ax = axs[1]\n",
    "  ax.set_title('Accuracy', fontsize = 25)\n",
    "  ax.grid(color = 'black', alpha = 0.5, linestyle = 'dashed', linewidth = 0.5)\n",
    "  ax.plot(train_acc, label = 'acc_train', color = 'black', linestyle = '-')\n",
    "  ax.plot(val_acc, label = 'acc_valid', color = 'orange')\n",
    "  ax.set_xlabel('Epochs', fontsize = 13)\n",
    "  plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Yy2nIGFLlPFS"
   },
   "source": [
    "## Pregunta 1: Construyendo una red neuronal desde cero\n",
    "\n",
    "Para este ejercicio construiremos una red neuronal simple desde cero, la entrenaremos y veremos sus resultados. Para esto utilizaremos un dataset de juguete construido a partir de datos que provienen de dos Gaussianas multivariadas. El objetivo es generar un modelo capaz de clasificar entre estas dos Gaussianas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "colab_type": "code",
    "id": "i15G9DkHlPFT",
    "outputId": "2f28067d-c6a5-4336-f27d-b08b001cc5e8"
   },
   "outputs": [],
   "source": [
    "#Create Distribution Multivariant\n",
    "mean1 = [0.5, 0.5]\n",
    "cov1 = [[0.02, 0], [0, 0.01]]\n",
    "x1 = np.random.multivariate_normal(mean1, cov1, 500)    #two features in x1\n",
    "y1 = np.zeros((500,1), dtype='uint')    #one output y1  all ZEROS\n",
    "\n",
    "mean2 = [0.75, 0.8]\n",
    "cov2 = [[0.02, 0.01], [0.01, 0.02]]\n",
    "x2 = np.random.multivariate_normal(mean2, cov2, 500)   ##two features in x2\n",
    "y2 = np.ones((500,1), dtype='uint')   #one output y2 all ONES\n",
    "\n",
    "x, y = np.concatenate((x1, x2), axis=0), np.concatenate((y1, y2), axis=0)\n",
    "\n",
    "plt.plot(x1[:,0], x1[:,1], '*', label='Gaussiana multivariada 1', c='g', alpha=0.4)\n",
    "plt.plot(x2[:,0], x2[:,1], '.', label='Gaussiana multivariada 2', c='b', alpha=0.4)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ke8n7fj-lPFZ"
   },
   "source": [
    "1.1 En este paso construiremos la red neuronal perceptron multicapa que utilizaremos para clasificar los datos provenientes de las dos Gaussianas multivariadas. Para esto utilizaremos las siguientes especificaciones:\n",
    "1. Una capa oculta de 8 neuronas y una capa de salida de 1 neurona.\n",
    "2. Función de activación sigmoide.\n",
    "3. Función de divergencia de entropía cruzada binaria.\n",
    "\n",
    "\n",
    "**Hint:**\n",
    "Existen muchas formas de construir esta red y calcular sus parámetros, pero por simplicidad se sugiere un esquema como el que sigue (puede servir de guía, pero puede programar la red como estime conveniente):\n",
    "\n",
    "```\n",
    "class PerceptronMulticapa:\n",
    "    '''\n",
    "    primero definir una clase para la red neuronal, la cual recibe como parámetrs las dimensiones de entrada, \n",
    "    oculta y salida, asi como el learning rate necesario para la actualización de los parámetros.\n",
    "    '''\n",
    "    def __init__(self, inp_dim, hidden_dim, out_dim, lr):\n",
    "        #definición de los parámetros de la red.\n",
    "        # W1 = aleatorio\n",
    "        # W2 = aleatorio\n",
    "     \n",
    "    def forward(self, x, y):\n",
    "        '''\n",
    "        Acá se calcula el valor predicho por la red, así como los valores intermedios de la capa oculta.\n",
    "        Se deben tener en cuenta las funciones de activación y de costo. Estas y sus derivadas pueden \n",
    "        programarse como funciones aparte.\n",
    "        '''\n",
    "        # X^1 = f(W^1X^0)\n",
    "        # X^2 = f(W^2X^1)\n",
    "        # Loss(y,X^2)\n",
    "\n",
    "\n",
    "    def backward(self, x, y):\n",
    "        '''\n",
    "        Acá se calculan los nuevos parámetros de la red utilizando el método de backpropagation \n",
    "        y gradient descent.\n",
    "        '''\n",
    "        # dW2 = Loss' * f(W^2X^1)' * (W^2X^1)'\n",
    "        # dW1 = ...\n",
    "\n",
    "\n",
    "        # W2 -= learnng_rate * dW2 \n",
    "        # W1 -= learnng_rate * dW1\n",
    "```\n",
    "\n",
    "(10 pts.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1QXVhV11JfNF"
   },
   "outputs": [],
   "source": [
    "#CONJUNTO DE FUNCIONES DE ACTIVACIÓN CON SU DERIVADA\n",
    "#FUNCION SIGMOIDE\n",
    "'''\n",
    "def sigmoid(x):\n",
    "    \"Numerically stable sigmoid function.\"\n",
    "    if x >= 0:\n",
    "        z = exp(-x)\n",
    "        return 1 / (1 + z)\n",
    "    else:\n",
    "        # if x is less than zero then z will be small, denom can't be\n",
    "        # zero because it's 1+z.\n",
    "        z = exp(x)\n",
    "        return z / (1 + z)\n",
    "'''\n",
    "#funcion sigmoide\n",
    "from scipy.special import expit\n",
    "\n",
    "#derivada sigmoide\n",
    "def d_sigmoid(z, y):\n",
    "  derivate = np.zeros((z.shape[0], z.shape[1], z.shape[1])) #tengo una tercera dimension al inicio para guardar el jacobiano de cada uno de los z de cada dato\n",
    "  for i in range(z.shape[0]): #recorro datos\n",
    "    for j in range(z.shape[1]): #recorro neurona\n",
    "      for k in range(z.shape[1]): \n",
    "        if(j == k):  #jacobiano de una funcion scalar es una matriz diagonal\n",
    "          derivate[i][j][k] = expit(z[i][j]) * (1 - expit(z[i][j]))   \n",
    "        else:\n",
    "          derivate[i][j][k] = 0 #funcion de activacion escalar   \n",
    "  return derivate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tArbkhEXJjYU"
   },
   "outputs": [],
   "source": [
    "#CONJUNTO DE LOSS CON SU DERIVADA\n",
    "#### LOSS Binary crossentropy\n",
    "def loss_binary_crossentropy(y_real, y_predicho):\n",
    "  loss = []\n",
    "  for i in range(y_real.shape[1]):\n",
    "    if(y_real[0][i] == 1):\n",
    "      loss.append(- np.log(y_predicho[0][i]))\n",
    "    else:  #y real es igual a cero\n",
    "      loss.append(- np.log(1.0 - y_predicho[0][i]))\n",
    "  loss = np.array(loss)\n",
    "  return loss.mean()\n",
    "\n",
    "#derivada binary crossentropy\n",
    "def derv_loss_binary_crossentropy(y_pred, y_real):   #entrada tengo el orden (N°neuronas ultima capa, N°Datos)  (1, N datos)\n",
    "#CAMBIO EL ORDEN PARA QUE QUEDE EN MATRIZ(N°DATOS, N°NEURRONAS DE ULTIMA CAPA) para poder hacer la retroprogación del error\n",
    "  y_pred = y_pred.T\n",
    "  y_real = y_real.T\n",
    "  derv = np.zeros((y_pred.shape[0], 1))\n",
    "  for i in range(y_pred.shape[0]):\n",
    "    if(y_real[i] == 1):\n",
    "      derv[i] = -1/y_pred[i]\n",
    "    else:\n",
    "      derv[i] = 1/(1 - y_pred[i])\n",
    "  return derv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YinTXrr1Z1p_"
   },
   "outputs": [],
   "source": [
    "#funciones intermedias para realizar la red neuronal\n",
    "#SUFFLE DE LOS DATOS DE ENTRENAMIENTO\n",
    "def suffle_data(x, y):   \n",
    "  #x e y tiene que estar en formato de matrices de train_test_split (dato, feature), (dato, salida)\n",
    "  #RETURN PARA RED NEURONAL (FEATURE, DATO)  ,  (SALIDA, DATO)\n",
    "  data = np.hstack(( x, y ))\n",
    "  data_index = np.hstack(( np.array(    [[i] for i in range(x.shape[0])]   ) , data ))\n",
    "  np.random.shuffle(data_index)\n",
    "  x_shuffle = data_index[:, 1: data_index.shape[1]-y.shape[1] ].T\n",
    "  y_shuffle = data_index[:, 1+x.shape[1]:].T\n",
    "  return x_shuffle, y_shuffle\n",
    "\n",
    "#OBTENER BATCH DE LOS DATOS DE ENTRENAMIENTO\n",
    "def get_batch(x, y, size_batch, i):   #entrada y salida en formato (feature, data)\n",
    "  return x[: , i*size_batch : i*size_batch+size_batch ], y[: , i*size_batch : i*size_batch+size_batch ]\n",
    "\n",
    "#OBTENER LA PREDICCIÓN DESDE LAS PROBABILIDADES\n",
    "def get_pred(y_horizontal_2d):  \n",
    "  y_horizontal = y_horizontal_2d.reshape(y_horizontal_2d.shape[1]).copy()\n",
    "  for i in range(y_horizontal.shape[0]):\n",
    "    if(y_horizontal[i] >= 0.5):\n",
    "      y_horizontal[i] = 1\n",
    "    else:\n",
    "      y_horizontal[i] = 0\n",
    "  return y_horizontal\n",
    "\n",
    "#CALCULAR GRADIENTE DE Z (DERIVADA DIVERGENCIA POR MATRIZ JACOBIANA DE LA FUNCION DE ACTIVACIÓN CON RESPECTO A Z)\n",
    "def calculate_grad_Z(grad_y, d_f_act, z, y):\n",
    "  #para evaluar el jacobiano en cada punto de Z, Z tiene que estar en el formato traspuesto ocupado en back (100X1) 100 datos x 1 neurona\n",
    "  z = z.T\n",
    "  y = y.T\n",
    "  jacobian = d_f_act(z, y)  #jacobiano que obtengo es una matriz (data, (numero neuronas, numero neuronas))   #matriz de 3 dimensiones (datos, 4x4) 4 neuronas  \n",
    "  grad_Z = []\n",
    "  for i in range(grad_y.shape[0]):\n",
    "    grad_Z.append(grad_y[i] @ jacobian[i])\n",
    "  grad_Z = np.array(grad_Z).reshape(grad_y.shape[0], grad_y.shape[1])  #devuelve gradiente de z de igual tamaño al gradiente de y\n",
    "  return grad_Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ad9YSQUmawH1"
   },
   "source": [
    "#### MODELO FULLY CONNECTED\n",
    "###### Se pueden crear N layers fully connected, de cualquier tipo, solo se necesita definir una función para las funciones de activación y losses con sus respectivas derivadas en formato matricial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E6yb6KHEJoJP"
   },
   "outputs": [],
   "source": [
    "class model_nn():\n",
    "  def __init__(self):\n",
    "    #inicializar lista donde se guardaran los z, y, w, funciones de costos que utiliza el algoritmo para optimizar\n",
    "    self.Z = [None]\n",
    "    self.Y = [] #el y sub cero tiene un valor\n",
    "    self.W = [] #dejo vacia la lista porque al crear los primeros W ahi voy a agregar el None en la primera capa\n",
    "    self.b = []\n",
    "    self.f_act = [None]\n",
    "    self.d_f_act = [None]\n",
    "\n",
    "    #inicializar para hacer forward de conjunto de validación\n",
    "    self.Z_valid = [None]\n",
    "    self.Y_valid = [None]\n",
    "  \n",
    "  def reset_params(self):#reiniciar para hacer otra corrida de forward con valores W guardados y las funciones de activacion. hacer nuevo paso del gradiente\n",
    "    #reiniciar valores que tiene cada neurona al hacer cada paso del gradiente\n",
    "    self.Z = [None]\n",
    "    self.Y = [] \n",
    "    self.Z_valid = [None]\n",
    "    self.Y_valid = []\n",
    "\n",
    "  #agregar layer en formato de lista, cada elemento de la lista es una capa en formato de una matriz de numpy con las dimensiones correspondientes\n",
    "  def add_layer(self,  n_neuron_actual, n_features = None, activation_function = None, derivate_act_function = None):\n",
    "    if self.W: #no esta vacia la red neuronal\n",
    "      self.W.append(np.random.normal(size = (n_neuron_actual, self.n_neuron_last_layer)))\n",
    "      self.b.append(np.random.normal(size = (n_neuron_actual, 1)))   \n",
    "      self.f_act.append(activation_function)\n",
    "      self.d_f_act.append(derivate_act_function) \n",
    "      self.n_neuron_last_layer = n_neuron_actual\n",
    "\n",
    "    else:  #esta vacia la red \n",
    "      self.W.append(None)\n",
    "      self.b.append(None)\n",
    "      self.W.append(np.random.normal(size = (n_neuron_actual, n_features)))\n",
    "      self.b.append(np.random.normal(size = (n_neuron_actual, 1)))   \n",
    "      self.f_act.append(activation_function) \n",
    "      self.d_f_act.append(derivate_act_function) \n",
    "      self.n_neuron_last_layer = n_neuron_actual\n",
    "\n",
    "  def add_loss(self, loss_function, derivate_loss_function):  #definir la loss a utilizar\n",
    "    self.loss = loss_function\n",
    "    self.d_loss = derivate_loss_function\n",
    "\n",
    "  def forward_pass(self, data_train):  #forward pass del cojunto de entrenamiento. Batch del conjunto de entrenamiento\n",
    "    self.Y.append(data_train)  #asigno y[k = 0] primera capa\n",
    "    for k in range(1, len(self.W)):  #para cada una de las capas \n",
    "      #expandir dimensiones de y agregando el 1 y aumentar dimension de pesos al juntar las matrices weights y bias\n",
    "      Y_exp = np.vstack((np.ones(data_train.shape[1]), self.Y[k-1])) #en cada iteracion agrego el 1 \n",
    "      W_exp = np.hstack(( self.b[k] , self.W[k] ))  #juntar weights y bias auxiliar para calculos\n",
    "      self.Z.append(W_exp @ Y_exp)\n",
    "      self.Y.append( self.f_act[k](self.Z[k]) )\n",
    "\n",
    "  def forward_pass_2(self, data_train, data_valid):  #forward pass del batch de entrenamiento y de todos los datos de validación\n",
    "    self.Y.append(data_train)  \n",
    "    self.Y_valid.append(data_valid)\n",
    "    for k in range(1, len(self.W)): \n",
    "      Y_exp = np.vstack((np.ones(data_train.shape[1]), self.Y[k-1]))\n",
    "      Y_exp_valid = np.vstack((np.ones(data_valid.shape[1]), self.Y_valid[k-1]))\n",
    "      W_exp = np.hstack(( self.b[k] , self.W[k])) \n",
    "      self.Z.append(W_exp @ Y_exp)\n",
    "      self.Z_valid.append(W_exp @ Y_exp_valid)\n",
    "      self.Y.append( self.f_act[k](self.Z[k]))\n",
    "      self.Y_valid.append( self.f_act[k](self.Z_valid[k]))\n",
    "\n",
    "  def backward_pass(self, y_real):\n",
    "    self.N = len(self.Y) - 1  #cantidad de capas creadas (capa 0, capa 1, capa2, capa3)\n",
    "    \n",
    "    #crear gradientes en cada back y actualizarlos en la iteración\n",
    "    self.grad_Y = [0 for i in range(len(self.Y))] #inicializar gradientes de las variables que necesito. Inicializar variables para cada layer\n",
    "    self.grad_Z = [0 for i in range(len(self.Y))] \n",
    "    self.grad_W = [0 for i in range(len(self.Y))] \n",
    "    self.grad_b = [0 for i in range(len(self.Y))] \n",
    "\n",
    "    while (self.N > 0):  #desde la ultima capa a la primera\n",
    "      if (self.N  == len(self.Y) - 1):   #solo para la ultima capa\n",
    "        self.grad_Y[self.N] =  model.d_loss(self.Y[self.N], y_real)   #derivada de la Loss\n",
    "      else:  #caso contrario que no estoy en la ultima capa\n",
    "        self.grad_Y[self.N] = self.grad_Z[self.N + 1] @ self.W[self.N + 1]\n",
    "\n",
    "      #calcular gradiente de z necesito la derivada de la funcion de activacion evaluada en Z\n",
    "      #en escalar solo necesito evaluar en z, en vectorial softmax necesito el valor de z ademas de valor de y\n",
    "      self.grad_Z[self.N] =  calculate_grad_Z(self.grad_Y[self.N], self.d_f_act[self.N],self.Z[self.N], self.Y[self.N])\n",
    "      self.grad_W[self.N] = self.Y[self.N - 1] @ self.grad_Z[self.N]\n",
    "      self.grad_b[self.N] = np.ones((1, self.Y[0].shape[1])) @ self.grad_Z[self.N]\n",
    "      self.N -=1\n",
    "\n",
    "  def update_weights(self, lr):\n",
    "    for k in range(1, len(self.W)):\n",
    "      self.W[k] = self.W[k] - ( lr /  self.Y[0].shape[1] ) * self.grad_W[k].T\n",
    "      self.b[k] = self.b[k] - ( lr /  self.Y[0].shape[1] ) * self.grad_b[k].T\n",
    "\n",
    "  def save_initial_weights(self):\n",
    "    self.initial_W = self.W.copy()\n",
    "    self.initial_b = self.b.copy()\n",
    "\n",
    "  def reset_weights_to_initial(self):\n",
    "    self.W = self.initial_W.copy()\n",
    "    self.b = self.initial_b.copy()\n",
    "\n",
    "\n",
    "  def train_nn(self, X_train, y_train, X_valid, y_valid, num_batches, size_batch, lr):\n",
    "    loss_train = []    #métricas en train y validacion\n",
    "    acc_train = []\n",
    "    loss_valid = []\n",
    "    acc_valid = []\n",
    "\n",
    "    for i in range(epoch):  #para cada época\n",
    "      loss_batch_train = 0   #incializar el contador de la loss en train para luego promediar\n",
    "      acc_batch_train = 0   #inicializar el contador del acc en train para luego promediar\n",
    "      #shuffle de todo el train set\n",
    "      X_shuffled, y_shuffled = suffle_data(X_train, y_train)  #obtengo el suffled en formato para nn (feature, data)\n",
    "\n",
    "      for j in range(num_batches):  #para cada batch\n",
    "        X_train_batch, y_train_batch = get_batch(X_shuffled, y_shuffled, size_batch , j)  #obtener el batch del suffle de datos \n",
    "\n",
    "        ###FORWARD PASS\n",
    "        #IF SI ESTOY EN EL ULTIMO PASO DE GRADIENTE DE LA ÉPOCA ACTUALIZO LA LOSS DE TRAIN Y DE VALID porque como lo programe se pueden guardar los pesos\n",
    "        #pero hacer el forward con pesos guardados no esta programado, no asi hacer el forward inmediatamente al calcular los pesos haciendo que para los objetivos\n",
    "        #del problema tampoco sea necesario guardar los pesos de cada layer en cada paso de gradiente de cada epoca\n",
    "        if(j == num_batches - 1):  #estoy en el ultimo paso del gradiente de la época hago forward de train y valid   \n",
    "          self.forward_pass_2(X_train_batch, X_valid)\n",
    "          loss_batch_train += self.loss(y_train_batch, self.Y[-1])\n",
    "          acc_batch_train +=  accuracy_score( y_train_batch.reshape(y_train_batch.shape[1]),  get_pred( self.Y[-1] ) )  #para evaluar el acc tiene que estar en formato(x, ) y train y valid en número entero\n",
    "          #valid\n",
    "          loss_valid.append(self.loss(y_valid, self.Y_valid[-1]))\n",
    "          acc_valid.append(accuracy_score( y_valid.reshape(y_valid.shape[1]), get_pred(self.Y_valid[-1]) )  )   #real y predicho\n",
    "\n",
    "        else:  #no esoty en el último paso del gradiente de la época y solo hago el forward de train\n",
    "          self.forward_pass(X_train_batch)  \n",
    "          loss_batch_train += self.loss(y_train_batch, self.Y[-1])   #calcular la loss de la regresión en ese forward con el batch\n",
    "          acc_batch_train +=  accuracy_score( y_train_batch.reshape(y_train_batch.shape[1]),  get_pred( self.Y[-1] )  )\n",
    "\n",
    "        ###BACK PASS\n",
    "        self.backward_pass(y_train_batch)  #calcular gradientes\n",
    "        self.update_weights(lr)   #actualizo los pesos\n",
    "\n",
    "        #TERMINO UN PASO DE GRADIENTE, reinicio los y, z que estan guardados en las neuronas y los borro para almacenar los valores de un nuevo paso de gradiente\n",
    "        self.reset_params()      \n",
    "\n",
    "      #promedio de la loss y acc de train en cada época\n",
    "      loss_train.append(  loss_batch_train/num_batches )\n",
    "      acc_train.append(acc_batch_train/num_batches)\n",
    "    return loss_train, acc_train, loss_valid, acc_valid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r0aql7l6lPFe"
   },
   "source": [
    "1.2 Utilizando los datos proveídos construya los un conjunto de entrenamiento(60%), validación (20%) y test (20%). Asegurese de que al menos en el conjunto de entrenamento los datos se encuentren aleatorizados respecto a su clase. \n",
    "\n",
    "(3 pts.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "snYy6Wv7lPFe",
    "outputId": "fb4d1885-5e51-4f68-82ee-997c03d221d8"
   },
   "outputs": [],
   "source": [
    "X_tv, X_test, y_tv, y_test = train_test_split(x, y, test_size = 200, random_state = 1)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_tv, y_tv, test_size = 200, random_state = 1)\n",
    "print('Dimensiones de los diferentes conjuntos')\n",
    "print('Train size X', X_train.shape)\n",
    "print('Train size y', y_train.shape)\n",
    "\n",
    "print('\\nValid size X', X_valid.shape)\n",
    "print('Valid size y', y_valid.shape)\n",
    "\n",
    "print('\\nTest size X', X_test.shape)\n",
    "print('Test size y', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "fGZ3aggNMsPR",
    "outputId": "0d1be5c4-40e7-4e7e-e4ca-c53612ec8bdf"
   },
   "outputs": [],
   "source": [
    "print('Viualización conjunto de Train y Test')\n",
    "visualize_train_valid_test(X_train, X_valid, X_test, y_train, y_valid, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZWT-q78HlPFi"
   },
   "source": [
    "1.3 Entrene la red neuronal construida en el primer paso, variando el learning rate en cada entrenamiento. Se pide probar con las siguientes alternativas: [1, 0.1, 0.01, 0.001, 0.0001, 0.00001].\n",
    "Por cada entrenamiento utilize al menos 300 épocas y muestre la curvas de aprendizaje para accuracy y loss. \n",
    "Las curvas deben ser construidas utilizando el conjunto de entrenamiento y de validación.\n",
    "\n",
    "¿Cuál es la mejor alternativa de learning rate para el problema propuesto? ¿Por qué? \n",
    "\n",
    "(7 pts.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "ZqTkm7ZaGaxk",
    "outputId": "9a5eed27-063a-4e64-87d1-f435b7dd65b8"
   },
   "outputs": [],
   "source": [
    "#ordenar dimensiones para que coinciden con codigo creado \n",
    "#(train = data, feature)   despues a un suffle y a un get batch donde se dimensiona de forma correcta para la red neuronal\n",
    "#(valid = feature, data) -> formato de ingreso a la red neuronal\n",
    "X_train_mlp = X_train.copy()\n",
    "y_train_mlp = y_train.reshape(y_train.shape[0], 1).copy()\n",
    "X_valid_mlp = X_valid.T.copy()\n",
    "y_valid_mlp = y_valid.reshape(y_valid.shape[0], 1).T.copy()\n",
    "\n",
    "print('Train size X', X_train_mlp.shape)\n",
    "print('Train size y', y_train_mlp.shape)\n",
    "\n",
    "print('\\nValid size X', X_valid_mlp.shape)\n",
    "print('Valid size y', y_valid_mlp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "xgTDnJtIJROJ",
    "outputId": "25ae6f14-58dc-4bf9-a83b-03534e56d1ae"
   },
   "outputs": [],
   "source": [
    "print('Create Fully Connected Neural Network')\n",
    "model = model_nn()\n",
    "model.add_layer(8, 2, activation_function = expit, derivate_act_function = d_sigmoid) #crear capa con 8 neuronas y la anterior(inicial) tiene 2 features\n",
    "model.add_layer(1, activation_function = expit, derivate_act_function = d_sigmoid)  #crear capa con 1 neurona\n",
    "model.add_loss(loss_binary_crossentropy, derv_loss_binary_crossentropy)  #crear loss\n",
    "model.save_initial_weights()   #guardar los pesos iniciales para hacer la comparativa de lr utilizando los mismos pesos\n",
    "\n",
    "print('\\nWeights')\n",
    "print('Weights First Layer', model.W[1].shape)\n",
    "print('Weights Second Layer', model.W[2].shape)\n",
    "print('\\nActivation function', model.f_act)\n",
    "print('Derivate activation function', model.d_f_act)\n",
    "print('Loss:', model.loss, 'Derivate_loss:', model.d_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "liHRlxnkJRHn"
   },
   "outputs": [],
   "source": [
    "#Parameters to train\n",
    "epoch = 300\n",
    "size_batch = 30  #número de observaciones en cada batch\n",
    "num_batches = int(X_train_mlp.shape[0] / size_batch)       #cantidad de batches, mejor un numero exacto para evitar no pasar algunos datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "kLWpefMStASU",
    "outputId": "e4f35f9f-6546-4da0-df28-7349bdc43077"
   },
   "outputs": [],
   "source": [
    "print('Entrenando la red neuronal con diferentes lr utilizando la misma configuración inicial aleatoria de los pesos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "colab_type": "code",
    "id": "oJPf9-_gyTwI",
    "outputId": "96806794-ddff-43c3-be21-d73b4d397a0b"
   },
   "outputs": [],
   "source": [
    "print('lr = 1')\n",
    "loss_train, acc_train, loss_valid, acc_valid = model.train_nn(X_train_mlp, y_train_mlp, X_valid_mlp, y_valid_mlp, num_batches, size_batch, 1)\n",
    "visualize_results(loss_train, loss_valid, acc_train, acc_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "colab_type": "code",
    "id": "Cky689lYtAMB",
    "outputId": "4a19effe-a8a6-49c4-8598-cf8d7e72b883"
   },
   "outputs": [],
   "source": [
    "print('lr = 0.1')\n",
    "model.reset_weights_to_initial()\n",
    "loss_train, acc_train, loss_valid, acc_valid = model.train_nn(X_train_mlp, y_train_mlp, X_valid_mlp, y_valid_mlp, num_batches, size_batch, 0.1)\n",
    "visualize_results(loss_train, loss_valid, acc_train, acc_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "colab_type": "code",
    "id": "vpFShIQqtAE6",
    "outputId": "b3db96a9-3161-461c-8493-bf257da49a1b"
   },
   "outputs": [],
   "source": [
    "print('lr = 0.01')\n",
    "model.reset_weights_to_initial()\n",
    "loss_train, acc_train, loss_valid, acc_valid = model.train_nn(X_train_mlp, y_train_mlp, X_valid_mlp, y_valid_mlp, num_batches, size_batch, 0.01)\n",
    "visualize_results(loss_train, loss_valid, acc_train, acc_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "colab_type": "code",
    "id": "9mRaptkoytnh",
    "outputId": "889ee995-bec7-4981-de1d-2ca8f9bf2ed1"
   },
   "outputs": [],
   "source": [
    "print('lr = 0.001')\n",
    "model.reset_weights_to_initial()\n",
    "loss_train, acc_train, loss_valid, acc_valid = model.train_nn(X_train_mlp, y_train_mlp, X_valid_mlp, y_valid_mlp, num_batches, size_batch, 0.001)\n",
    "visualize_results(loss_train, loss_valid, acc_train, acc_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "colab_type": "code",
    "id": "hoht-1DIy1Q0",
    "outputId": "fef8ad71-e114-4044-f1fe-ad61f59446f7"
   },
   "outputs": [],
   "source": [
    "print('lr = 0.0001')\n",
    "model.reset_weights_to_initial()\n",
    "loss_train, acc_train, loss_valid, acc_valid = model.train_nn(X_train_mlp, y_train_mlp, X_valid_mlp, y_valid_mlp, num_batches, size_batch, 0.0001)\n",
    "visualize_results(loss_train, loss_valid, acc_train, acc_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "colab_type": "code",
    "id": "CgSML9Uoy6AF",
    "outputId": "84b338d8-b8be-4358-a4b7-909d31e654ae"
   },
   "outputs": [],
   "source": [
    "print('lr = 0.00001')\n",
    "model.reset_weights_to_initial()\n",
    "loss_train, acc_train, loss_valid, acc_valid = model.train_nn(X_train_mlp, y_train_mlp, X_valid_mlp, y_valid_mlp, num_batches, size_batch, 0.00001)\n",
    "visualize_results(loss_train, loss_valid, acc_train, acc_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kDqpotoizJ3i"
   },
   "source": [
    "**Respuesta:** La mejor alternativa de Learning rate a utilizar considerando el mismo conjunto inicial (aleatorio normal) de los pesos y bias (para poder comparar las diferentes redes neuronales con los mismos parámetros iniciales a excepción del LR y de los datos en el batch de entrenamiento), corresponde a una configuración de LR = 0.1 ó 0.01 ya que para ambos hiperparámetros se observan curvas de loss y accuracy muy similares, con un descenso más o menos suave de la curva de loss mostrando que explora una mayor parte de la curva no como con configuraciones, por ejemplo de LR = 1 la cual muestra un descenso brusco en las primeras épocas para posteriormente mostrar muchas fluctuaciones en la loss con respecto a un punto mínimo debido al alto learning rate, mientras que un LR = 0.001 y 0.00001 es muy pequeño y no logra hacer un descenso suficientemente rápido de la curva de loss.\n",
    "\n",
    "Por otro lado, con el LR de 0.1 ó 0.01, las curvas para train y valid (tanto de loss como de accuracy) son muy similares, esto indica que la red neuronal se ajusta bien tanto para los datos de entrenamiento como validación, lo cual se puede explicar al observar los gráficos de los diferentes conjuntos de train y valid (puntos x e y) y darse cuenta que ambos conjuntos son muy similares por lo cual una red que se ajusta bien a un conjunto de entrenamiento, producirá resultados muy similares con un conjunto de validación muy similar al primero de entrenamiento. Del mismo modo, al ser curvas muy similares no se observa un problema de overffiting ni de underfitting ya que se consiguen resultados muy buenos con una loss muy cercana a cero y un accuracy muy cercano a 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FI_9dYrwlPFm"
   },
   "source": [
    "## Pregunta 2: Entendiendo las operaciones convolucionales\n",
    "\n",
    "En este apartado vamos a investigar de manera gráfica cuál es el resultado de una operación de convolución, que corresponde a la capacidad de una red convolucional de generar representaciones a partir de una imágen.\n",
    "\n",
    "<img src=\"images/cute_cat.jpeg\" alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "https://github.com/joseortegalabra/MLBI/blob/master/Tarea2/images/cute_cat.jpeg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OOMsifZz29dG"
   },
   "outputs": [],
   "source": [
    "#funciones usadas en pregunta 2\n",
    "from skimage import io\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.ndimage.filters import convolve\n",
    "\n",
    "def visualize_conv_filter(original, convolution, left_name, right_name):\n",
    "  fig, axs = plt.subplots(1,2, figsize = ((14,14)))\n",
    "  axs[0].imshow(original)\n",
    "  axs[0].set_title(left_name)\n",
    "\n",
    "  axs[1].imshow(convolution)\n",
    "  axs[1].set_title(right_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AWL4EynRlPFm"
   },
   "source": [
    "2.1 Cargue a imagen anterior como una matriz NumPy. Luego normalice la imagen de tal forma que sus valores estén dentro del rango [-0.5, 0.5]\n",
    "\n",
    "(3 pts.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 884
    },
    "colab_type": "code",
    "id": "qD1nQ8B0lPFn",
    "outputId": "eeabf52e-9667-4632-9122-1c7b40093921"
   },
   "outputs": [],
   "source": [
    "cute_cat_path = 'https://raw.githubusercontent.com/joseortegalabra/MLBI/master/Tarea2/images/cute_cat.jpeg'\n",
    "print('CUTE_CAT NUMPY MATRIX\\n')\n",
    "cute_cat = io.imread(cute_cat_path) /255.0 # imread lee las imagenes con los pixeles codificados como enteros \n",
    "cute_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "HltHVL_rW_6l",
    "outputId": "eead037f-175d-42e8-99c9-494d893ae99c"
   },
   "outputs": [],
   "source": [
    "h =  cute_cat.shape[0]  #Alto\n",
    "w =   cute_cat.shape[1]  #Ancho\n",
    "c =    cute_cat.shape[2]  #canal\n",
    "print('Alto Pixeles', h)  \n",
    "print('Ancho Pixeles', w)  \n",
    "print('Canales', c)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 348
    },
    "colab_type": "code",
    "id": "7Gr5yUIbZPvr",
    "outputId": "517cbc5a-a07f-40c4-e015-678e67fe03de"
   },
   "outputs": [],
   "source": [
    "print('VISUALIZE ORIGINAL CUTE_CAT\\n')\n",
    "io.imshow(cute_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 884
    },
    "colab_type": "code",
    "id": "UvCEu7oIp1lx",
    "outputId": "f54d1676-299b-4a12-e545-bb33ed23b896"
   },
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "cute_cat_scaled = scaler.fit_transform(cute_cat.reshape(h*w, c)) -0.5  # reshape the input so that each channel becomes one column - -0.5 to scale into -0.5,0.5\n",
    "cute_cat_scaled = cute_cat_scaled.reshape(h,w,c)\n",
    "print('CUTE_CAT NORMALIZED -0.5, 0.5\\n\\n', cute_cat_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "DKc8W13usHO2",
    "outputId": "9e731083-55ee-47dc-d4af-43ff7974d866"
   },
   "outputs": [],
   "source": [
    "print('Revisión Imagen Normalizada')\n",
    "print('Mínimo Valor de la imagen cute_cat_normalizada: ', np.min(cute_cat_scaled))\n",
    "print('Posición del mínimo valor en la matriz flattened cute_cat_normalizada: ' ,np.argmin(cute_cat_scaled) )\n",
    "print('Valor en esa posición:', cute_cat_scaled.reshape(w*h*c)[np.argmin(cute_cat_scaled)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 348
    },
    "colab_type": "code",
    "id": "jARKPtDgrhs1",
    "outputId": "d1ea9125-0fef-4ea9-8b87-11040fd4e137"
   },
   "outputs": [],
   "source": [
    "print('Visualize Scaler -0.5, 0.5 cute_cat')\n",
    "io.imshow(cute_cat_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jxN1lVhblPFr"
   },
   "source": [
    "2.2 Elija dos kernel de tamaño 3x3 (ej. border detection, sharpen, box blur kernels, etc. Estos los puede encontrar [acá](https://en.wikipedia.org/wiki/Kernel_(image_processing))) y convolucione la imagen con cada uno de ellos. Para esto puede ocupar su propia función de convolución o utilzar la función [scipy.ndimage.filters.convolve](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.ndimage.filters.convolve.html), utiizando borde constante.\n",
    "\n",
    "(6 pts.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 440
    },
    "colab_type": "code",
    "id": "rYKCRjX1xa1U",
    "outputId": "83cbd710-d90a-40a7-a8c3-9a9c3f740602"
   },
   "outputs": [],
   "source": [
    "print('\\nEDGE DETECTION\\n')\n",
    "#filters 2d\n",
    "edge_detection = np.array([\n",
    "                           [-1, -1, -1],\n",
    "                           [-1, 8 , -1],\n",
    "                           [-1, -1, -1]\n",
    "])\n",
    "#filter 3d\n",
    "filter_edge_detection = np.array([edge_detection,edge_detection,edge_detection])\n",
    "#image convolution\n",
    "conv_edge_detection = convolve(cute_cat_scaled, filter_edge_detection, mode = 'constant', cval = 0.0)\n",
    "#show convolution\n",
    "visualize_conv_filter(cute_cat_scaled, conv_edge_detection, 'Original Cue Cat', 'Edge Detection Filter Convolution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 440
    },
    "colab_type": "code",
    "id": "AFvfhbv900I5",
    "outputId": "efac434c-c4f3-4d90-aad6-7c74ec1c219d"
   },
   "outputs": [],
   "source": [
    "print('\\nSHARPEN DETECTION\\n')\n",
    "#filters 2d\n",
    "sharpen = np.array([\n",
    "                           [0, -1, 0],\n",
    "                           [-1, 5, -1],\n",
    "                           [0, -1, 0]\n",
    "])\n",
    "#filter 3d\n",
    "filter_sharpen = np.array([sharpen,sharpen,sharpen])\n",
    "#image convolution\n",
    "conv_sharpen = convolve(cute_cat_scaled, filter_sharpen, mode = 'constant', cval = 0.0)\n",
    "#show convolution\n",
    "visualize_conv_filter(cute_cat_scaled, conv_sharpen, 'Original Cute Cat', 'Sharpen Filter Convolution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "qrHOWauHQXQx",
    "outputId": "e50645d0-0cfb-4341-f392-5387962b6f78"
   },
   "outputs": [],
   "source": [
    "print('Se observa que el filtro utilizado devuelve una matriz de 3 canales (RGB), lo cual para una operación de convolusión en una CNN es incorrecto ya que un filtro debe de devolver una imagen de un solo canal')\n",
    "print('Dimensiones resultado convolusión',conv_sharpen.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "60CONVlGlPFx"
   },
   "source": [
    "2.3 Una vez convolucionada la imagen, convierta a cero los valores menores a cero. Para esto puede usar la función [np.clip](https://numpy.org/doc/1.18/reference/generated/numpy.clip.html). En otras palabras:\n",
    "```\n",
    "if x < 0:\n",
    "    x = 0\n",
    "else:\n",
    "    x = x\n",
    "\n",
    "```\n",
    "(2 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 406
    },
    "colab_type": "code",
    "id": "seOjb8nE6BCP",
    "outputId": "b51776de-57c9-4940-cefd-63b1de5d3f09"
   },
   "outputs": [],
   "source": [
    "print('Relu Edge Detection')\n",
    "relu_edge_detection = np.clip(conv_edge_detection, 0, 100)\n",
    "visualize_conv_filter(conv_edge_detection, relu_edge_detection, 'Edge Detection', 'Relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 406
    },
    "colab_type": "code",
    "id": "OiRciLnl6r-c",
    "outputId": "01fee561-69c0-4199-9657-93639002aff1"
   },
   "outputs": [],
   "source": [
    "print('Relu Sharpen')\n",
    "relu_sharpen = np.clip(conv_sharpen, 0, 100)\n",
    "visualize_conv_filter(conv_sharpen, relu_sharpen, 'Sharpen', 'Relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "325qgoYiCX2f",
    "outputId": "4353dfbb-1c21-4f6b-e6da-4fa4a075ec30"
   },
   "outputs": [],
   "source": [
    "print('Revisión Relu in Sharpen filter')\n",
    "print('Cantidad de elementos en el filtro', conv_sharpen.size)\n",
    "print('Cantidad de menores de ceros en el filtro antes de relu', conv_sharpen[conv_sharpen<0].size)\n",
    "print('Cantidad de menores de ceros despues de relu', relu_sharpen[relu_sharpen<0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IxPuOQ2UlPF0"
   },
   "source": [
    "2.4 Implemente por su cuenta la operación de agregación estadística Max-Pooling. Para esto utilice kernel_size=4 y stride=4.\n",
    "\n",
    "(6 pts.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I8Pg3M7D9N7r"
   },
   "outputs": [],
   "source": [
    "def pooling_(img, i, j, k, stride, filter_size):\n",
    "  max_ = 0 \n",
    "  for m in range(filter_size):  #moverme en altura\n",
    "    for n in range(filter_size): #moverme en el ancho\n",
    "      if(img[i*stride + m][j*stride + n][k] >= max_):\n",
    "        max_ = img[i*stride + m][j*stride + n][k]\n",
    "  return max_\n",
    "\n",
    "def max_pooling(img, height, weight, channel, filter_size, stride):\n",
    "  '''\n",
    "  Al aplicar los filtros de convolusion se obtiene una imagen de 3 canales, y no de 1 solo como una correcta convolusión en una CNN\n",
    "  Se aplica capa de pooling a cada uno de los canales reduciendo unicamente ancho y alto sin afecta la profunidad, como seria en una CNN\n",
    "  '''\n",
    "  pooling_size_height = math.floor(((height - filter_size)/stride)+1)\n",
    "  pooling_size_weight = math.floor(((weight- filter_size)/stride)+1)\n",
    "  print('Nuevas Dimensiones tras pooling')\n",
    "  print('Alto Pixeles', pooling_size_height,  'Ancho Pixeles', pooling_size_weight, 'Canales', channel, '\\n')\n",
    "  output_pooling = np.zeros((pooling_size_height, pooling_size_weight, channel))\n",
    "  for k in range(channel):\n",
    "    for i in range(pooling_size_height):\n",
    "      for j in range(pooling_size_weight):\n",
    "        output_pooling[i][j][k] = pooling_(img, i, j, k, stride, filter_size) \n",
    "  return output_pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 440
    },
    "colab_type": "code",
    "id": "SHyv_oHl3gOR",
    "outputId": "0114363b-84b6-4ff3-89b2-cc78a5d5155f"
   },
   "outputs": [],
   "source": [
    "pooling_edge_detection = max_pooling(relu_edge_detection, h, w, c, 4, 4)   #filtros pasados NO disminuyen las dimensiones de las imagenes\n",
    "visualize_conv_filter(relu_edge_detection, pooling_edge_detection, 'Relu Edge Detection', 'Pooling Edge Detection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 440
    },
    "colab_type": "code",
    "id": "rJAXXa9__Y6c",
    "outputId": "6b2e5b02-9294-4f6b-ccef-6d676acce33a"
   },
   "outputs": [],
   "source": [
    "pooling_sharpen = max_pooling(relu_sharpen, h, w, c, 4, 4)   #filtros pasados NO disminuyen las dimensiones de las imagenes\n",
    "visualize_conv_filter(relu_sharpen, pooling_sharpen, 'Relu Sharpen', 'Pooling Sharpen')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I_5hxwWolPF4"
   },
   "source": [
    "2.5 Muestre los resultados de las operaciones realizadas. ¿Qué puede decir acerca de ellos? ¿Cómo cree que estas operaciones pueden ser de utilidad para una red neuronal a la hora de realizar una tarea de clasificación?\n",
    "\n",
    "(3 pts.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qd5eO9K-lPF4"
   },
   "source": [
    "**Respuesta:** Se puede observar que los filtros cumplen con el objetivo detectando ya sea las características más notorias o los bordes de la imagen al aplicar un filtro de covolusión para luego aplicar una función relu (aplicar una función de transformación no lineal y permitir que la red neuronal pueda aprender relaciones no lineales) y posteriormente aplicar una capa de max pooling para extraer las características que más destacan de la imagen ademas de reducir las dimensiones de la imagen y por lo tanto reducir la cantidad de parámetros de la red neuronal.\n",
    "\n",
    "Las diferentes operaciones son de utilidad para una red neuronal porque permiten extraer las caraterísticas más importantes de la imagen y por lo tanto con estas características principales se puede diferenciar una imagen de otra y así permitir clasificar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lT_UXhN5lPF8"
   },
   "source": [
    "# Pregunta 3: Construyendo una Red Convolucional\n",
    "\n",
    "The Street View House Numbers ([SVHN](http://ufldl.stanford.edu/housenumbers/)) [1] es un conocido dataset, comunmente utilizado como benchmark para probar y comparar modelos de Machine Learning. Este dataset contiene imágenes de números de casa vistos desde la calle, los cuales se encuentran etiquetados y dicha etiqueta se asocia al dígito que se encuentra al centro de la imagen.\n",
    "\n",
    "<img src=\"images/svhn.png\" alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "https://github.com/joseortegalabra/MLBI/blob/master/Tarea2/images/svhn.png\n",
    "\n",
    "Los datos se encuentran públicamente disponibles a continuación:\n",
    "1. [train set](http://ufldl.stanford.edu/housenumbers/train_32x32.mat)\n",
    "2. [test set](http://ufldl.stanford.edu/housenumbers/test_32x32.mat)\n",
    "\n",
    "\n",
    "[1] Netzer, Y., Wang, T., Coates, A., Bissacco, A., Wu, B., Y Ng, A.: Reading Digits in Natural Images with Unsupervised Feature Learning. NIPS (2011)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ckaLs02rlPF8"
   },
   "source": [
    "3.1 Cargue las imagenes y etiquetas del set de entrenamiento y test como matrices NumPy. Las etiquetas van del número del '1' al '10', donde la décima etiqueta se asocia al valor 0. Cambie el valor de la etiqueta '10', para que cada etiqueta quede asociada al dígito que representa. Finalmente, grafique 9 imágenes del conjunto de entrenamiento.\n",
    "\n",
    "(3 pts.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SdtfGNXtHU-f"
   },
   "outputs": [],
   "source": [
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials\n",
    "import scipy.io\n",
    "\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)\n",
    "\n",
    "\n",
    "#data train\n",
    "train_32x32 = drive.CreateFile({'id':\"182W2-lNNJ4loIArl5WeUHH3jlMgvVy3g\"}) \n",
    "train_32x32.GetContentFile('train_32x32.mat') \n",
    "\n",
    "#data test\n",
    "test_32x32 = drive.CreateFile({'id':\"1RYtOpjb9W-WgCZTdJkhBkwbs0K-TGwFr\"})   # replace the id with id of file you want to access\n",
    "test_32x32.GetContentFile('test_32x32.mat')        # replace the file name with your file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oDUbOkjwHU4U"
   },
   "outputs": [],
   "source": [
    "set_train = scipy.io.loadmat('train_32x32.mat')\n",
    "set_test = scipy.io.loadmat('test_32x32.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "18Ff3OpdHUxj",
    "outputId": "a1f3d39c-2614-4f82-aa2d-93b3546a04bd"
   },
   "outputs": [],
   "source": [
    "X_train_valid = set_train['X']/255  #scale img 0 to 1\n",
    "y_train_valid = set_train['y']\n",
    "\n",
    "X_test = set_test['X']/255 #normalizar la imagen de 0 a 1, min max scaler\n",
    "y_test = set_test['y']\n",
    "\n",
    "print('Tamaño X_train_valid', X_train_valid.shape)\n",
    "print('Tamaño y_train_valid', y_train_valid.shape)\n",
    "print('Tamaño X_test', X_test.shape)\n",
    "print('Tamaño y_test', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ftQjIYmZgZdY"
   },
   "source": [
    "###### para poder procesar en keras debe de tener de ser un tensor del formato (n° imagenes, height, weight, channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mtzepu6Dfj8l"
   },
   "outputs": [],
   "source": [
    "def transform_to_tensor_keras(x):\n",
    "  x_transform = []\n",
    "  for i in range(x.shape[3]):\n",
    "    x_transform.append(x[:, :, :, i])\n",
    "  x_transform = np.array(x_transform)\n",
    "  return x_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "X1KViz8JgIdF",
    "outputId": "de5bc086-bc9f-467e-a6fa-b788d8b78cdf"
   },
   "outputs": [],
   "source": [
    "X_train_valid = transform_to_tensor_keras(X_train_valid)\n",
    "X_test = transform_to_tensor_keras(X_test)\n",
    "\n",
    "print('Tamaño X_train_valid', X_train_valid.shape)\n",
    "print('Tamaño y_train_valid', y_train_valid.shape)\n",
    "print('Tamaño X_test', X_test.shape)\n",
    "print('Tamaño y_test', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G6sqg-W3HTTz"
   },
   "outputs": [],
   "source": [
    "#transformar 10 a cero\n",
    "y_train_valid = np.where(y_train_valid < 10, y_train_valid, 0)\n",
    "y_test = np.where(y_test < 10, y_test, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hDYTf5QcHS3e"
   },
   "outputs": [],
   "source": [
    "#graficar 9 imagenes conjunto de entrenamiento\n",
    "def visualize_9_images(x, y, index):\n",
    "  fig, axs = plt.subplots(3, 3, figsize = ((14,14)))\n",
    "  for i in range(index.shape[0]):\n",
    "    for j in range(index.shape[1]):\n",
    "      axs[i][j].imshow(x[ index[i][j],:, :, :])\n",
    "      axs[i][j].set_title(y[ index[i][j] ])\n",
    "      axs[i][j].axis('off')\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 862
    },
    "colab_type": "code",
    "id": "_W9khmr-b92Z",
    "outputId": "45a9359e-b217-40c1-8edd-6fc831e7b1a3"
   },
   "outputs": [],
   "source": [
    "random_index = np.random.randint(low=0, high=X_train_valid.shape[0], size=(3,3))\n",
    "visualize_9_images(X_train_valid,y_train_valid , random_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VE2BY5UWlPGB"
   },
   "source": [
    "3.2 Para terminar el preprocesamiento de los datos, se pide que las etiquetas se presenten como variables categóricas. Esto es, para la etiqueta: \n",
    "```\n",
    "0 = [1, 0, ... , 0]\n",
    "1 = [0, 1, ... , 0]\n",
    "...\n",
    "N = [0, 0, ... , 1]\n",
    "```\n",
    "Además, se pide seleccionar un conjunto de validación para poder monitorear el sobreajuste del modelo sobre los datos de entrenamiento. Se pide que un 20% de los datos de entrenamiento sean utilizados para validación.\n",
    "\n",
    "(3 pts.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "utdFR-jvfFdH"
   },
   "outputs": [],
   "source": [
    "#obtener conjunto de train y valid\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_valid, y_train_valid, test_size = 0.2, random_state = 1)\n",
    "X_train = X_train\n",
    "X_valid = X_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 862
    },
    "colab_type": "code",
    "id": "JAsVupSpik6P",
    "outputId": "96a962a5-6cd5-446e-b86b-9a8cf427f0c0"
   },
   "outputs": [],
   "source": [
    "#visualización del conjunto de TRAIN\n",
    "visualize_9_images(X_train,y_train , np.random.randint(low=0, high=X_train.shape[0], size=(3,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cC5ylGTblPGB"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "y_train = np.array(pd.get_dummies(y_train.reshape(y_train.shape[0])))\n",
    "y_valid = np.array(pd.get_dummies(y_valid.reshape(y_valid.shape[0])))\n",
    "y_test  = np.array(pd.get_dummies(y_test.reshape(y_test.shape[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "xKhuWJvdfqH-",
    "outputId": "39533bbd-58a2-4f7e-bf3b-553da8cceaee"
   },
   "outputs": [],
   "source": [
    "#DIMENSIONES DE LOS DIFENTES CONJUNTOS DE TRAIN, VALID Y TEST\n",
    "print('Train size X', X_train.shape)\n",
    "print('Train size y', y_train.shape)\n",
    "\n",
    "print('\\nValid size X', X_valid.shape)\n",
    "print('Valid size y', y_valid.shape)\n",
    "\n",
    "print('\\nTest size X', X_test.shape)\n",
    "print('Test size y', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "tKyOKOjH-UtE",
    "outputId": "5ea4679f-5801-4bb5-cacb-3e20523786d9"
   },
   "outputs": [],
   "source": [
    "#EJEMPLO DE 10 primeras observaciones de Y en el conjunto de TEST\n",
    "y_test[:10, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ae1g4X4flPGE"
   },
   "source": [
    "3.3 Para construir red convolucional, se utilizará una modificación la famosa arquitectura arquitectura AlexNet, propuesta por Alex Krizhevsky [2] [(lectura obligatoria)](http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf).\n",
    "\n",
    "Se pide especificamente construir una red convoucional con la siguiente arquitectura:\n",
    "\n",
    "| Layer  | Operation|  Kernel |  Stride |  Activation | Maxpooling (stride) | Dropout (prob) |\n",
    "|---|---|---|---|---|---|---|\n",
    "|  1 | Convolución (32)  |  (3,3) |  1 |  ReLU |  No | No |\n",
    "|  2 |  Convolución (32) | (3,3)  |  1 | ReLU  |  Yes (2,2) | No |\n",
    "|  3 |  Convolución (64) | (3,3)  |  1 | ReLU  |  No | No |\n",
    "|  4 |  Convolución (64) | (3,3)  |  1 | ReLU  |  Yes (2,2) | Yes(0.25) |\n",
    "|  5 |  Densa (512) | -  | - | ReLU  | No | Yes(0.5) |\n",
    "|  5 |  Densa (10) | -  | - | Softmax  | No | No |\n",
    "\n",
    "\n",
    "Calcule el número de parámetros a dicha arquitectura.\n",
    "\n",
    "[2] Krizhevsky, A., Sutskever, I., Hinton, G.: ImageNet Classification with Deep Convolutional Neural Networks. NIPS (2012)\n",
    "\n",
    "(8 pts.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "HCpHHN6xlPGE",
    "outputId": "5a4b842e-b8f7-4e1b-82b5-5cfb1c242bf4"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten, Conv2D, MaxPooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bK4qNh5iLXeu"
   },
   "outputs": [],
   "source": [
    "modelCNN = Sequential()\n",
    "#first Covolutional layer   padding valid = no padding, padding same = zero pdding\n",
    "modelCNN.add(Conv2D(input_shape = (32, 32, 3), filters = 32, kernel_size =(3,3), strides = 1 , padding = 'valid' , activation = 'relu' , data_format = 'channels_last'))\n",
    "\n",
    "#second Convolutional layer\n",
    "modelCNN.add(Conv2D( filters = 32, kernel_size = (3,3), strides = 1 , padding = 'valid', activation = 'relu', data_format = 'channels_last'  ))\n",
    "modelCNN.add(MaxPooling2D( pool_size = (2,2),  strides = 1 , padding = 'valid', data_format= 'channels_last'))\n",
    "\n",
    "#third Convolutional layer\n",
    "modelCNN.add(Conv2D( filters = 64, kernel_size = (3,3), strides = 1,  padding = 'valid', activation = 'relu', data_format = 'channels_last' ))\n",
    "\n",
    "#fourth Convolutional layer\n",
    "modelCNN.add(Conv2D( filters = 64, kernel_size = (3,3), strides = 1, padding = 'valid', activation = 'relu' ))\n",
    "modelCNN.add(Dropout(0.25))   #dropout antes del maxpooling\n",
    "modelCNN.add(MaxPooling2D( pool_size = (2,2) , strides = 1, padding = 'valid', data_format = 'channels_last' ))\n",
    "\n",
    "#flatten layer\n",
    "modelCNN.add(Flatten())\n",
    "\n",
    "#Firts fully connected\n",
    "modelCNN.add(Dense(512, activation = 'relu'))\n",
    "modelCNN.add(Dropout(0.5))\n",
    "\n",
    "#Second fully connected\n",
    "modelCNN.add(Dense(10, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 527
    },
    "colab_type": "code",
    "id": "o0rpg92YUdAK",
    "outputId": "598a5034-503a-4125-f462-eac62bf93e98"
   },
   "outputs": [],
   "source": [
    "modelCNN.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rMz8j8y9lPGI"
   },
   "source": [
    "3.4 Entrene esta red neuronal utilizando la función de costos correspondiente y las siguientes consideraciones:\n",
    "\n",
    "1. Optimizador Adam (lr=0.0001)\n",
    "2. batch size = 64 \n",
    "3. Número de epocas = 10\n",
    "\n",
    "**OBS: debe tener en cuenta que el proceso de entrenamiento CPU puede demorar algunas horas. Si dispone de GPU (o utiliza coblab), puede acelerar este proceso y demorarse incluso minutos**\n",
    "\n",
    "(3 pts.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IR2BtBfxu4Kf"
   },
   "outputs": [],
   "source": [
    "#parameters\n",
    "lr = 0.0001\n",
    "num_epochs = 10\n",
    "batch = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "jtSnnED8w9ha",
    "outputId": "27cd67e6-ca9b-47fa-9542-ccbfcd289ba4"
   },
   "outputs": [],
   "source": [
    "#usar cuda\n",
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "CuTY30vsxWYL",
    "outputId": "9632e40c-26a7-45ce-9abe-6149e12231b9"
   },
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(\"Num GPUs:\", len(physical_devices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "UbYebITrlPGJ",
    "outputId": "03a2915f-f6ff-4c82-ed9b-b0585d1b6935"
   },
   "outputs": [],
   "source": [
    "opt = kr.optimizers.Adam(learning_rate=lr)\n",
    "modelCNN.compile(\n",
    "    loss = 'categorical_crossentropy',\n",
    "    optimizer = opt,\n",
    "    metrics = ['accuracy']\n",
    ")\n",
    "\n",
    "history_CNN = modelCNN.fit(\n",
    "    X_train,\n",
    "    y_train, \n",
    "    batch_size = batch,\n",
    "    verbose = 1,\n",
    "    epochs = num_epochs,\n",
    "    validation_data = (X_valid, y_valid)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aAOZcagwlPGM"
   },
   "source": [
    "3.5 Grafique dos curvas de aprendizaje, donde en el eje x se muestre el número de épocas y en el eje y se muestre la función de pérdida / accuracy respectivamente. Este gráfico debe contener los valores para el conjunto de entrenamiento, validación y el resultado final medido sobre el conjunto de test.\n",
    "\n",
    "(3 pts.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Rg4ATvI5sO8S"
   },
   "outputs": [],
   "source": [
    "score = modelCNN.evaluate(X_test, y_test, verbose=0)\n",
    "loss_test_CNN_mean = score[0]\n",
    "acc_test_CNN_mean = score[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AiGk8bgQlPGM"
   },
   "outputs": [],
   "source": [
    "def visualize_results_CNN(train_loss, val_loss, train_acc, val_acc, test_loss, test_acc):\n",
    "  #visualize acc and loss for each epoch in train and validation. \n",
    "  #visualize acc and loss mean in test (mean value for each epoch)\n",
    "  fig, axs = plt.subplots(nrows = 1, ncols = 2, figsize = ((14, 7)))\n",
    "  ax = axs[0]\n",
    "  ax.set_title('Loss', fontsize = 25)\n",
    "  ax.grid(color = 'black', alpha = 0.5, linestyle = 'dashed', linewidth = 0.5)\n",
    "  ax.plot(train_loss, label = 'loss_train', color = 'black', linestyle = '-')\n",
    "  ax.plot(val_loss, label = 'loss_valid', color = 'orange')\n",
    "  ax.plot(np.full( len(train_loss),test_loss), label = 'Mean loss_test', color = 'green')\n",
    "  ax.set_xlabel('Epochs', fontsize = 13)\n",
    "  ax.legend()\n",
    "\n",
    "  ax = axs[1]\n",
    "  ax.set_title('Accuracy', fontsize = 25)\n",
    "  ax.grid(color = 'black', alpha = 0.5, linestyle = 'dashed', linewidth = 0.5)\n",
    "  ax.plot(train_acc, label = 'acc_train', color = 'black', linestyle = '-')\n",
    "  ax.plot(val_acc, label = 'acc_valid', color = 'orange')\n",
    "  ax.plot(np.full(len(train_acc) ,test_acc), label = 'Mean acc_test', color = 'green')\n",
    "  ax.set_xlabel('Epochs', fontsize = 13)\n",
    "  plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 470
    },
    "colab_type": "code",
    "id": "H_MY7Ml2lcYX",
    "outputId": "793a6afe-406f-4a95-bf18-13799d12ed44"
   },
   "outputs": [],
   "source": [
    "visualize_results_CNN(history_CNN.history['loss'], history_CNN.history['val_loss'], history_CNN.history['accuracy'], \n",
    "                      history_CNN.history['val_accuracy'], loss_test_CNN_mean, acc_test_CNN_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ktA6h1tJ1kKo"
   },
   "source": [
    "**Respuesta:** Se puede observar que al entrenar la red neuronal en 10 épocas se observan resultados ideales que uno desearía obtener al entrenar una red neuronal, nose observa overfitting (al estar todas las curvas loss disminuyendo) ni underfitting (al tener loss muy baja y acc muy alto en todos los conjuntos de datos). Además la menor loss se da en el conjunto de entrenamiento que es lo deseable al entrenar la CNN con este conjunto de datos y obteniéndose loss un poco más altas con datos no visto como en el conjunto de validación y testeo con datos extraidos del \"mundo real\", como se dijo dichos resultados son los ideales que se desearía obtener en una NN a pesar de realizar muy pocas épocas (lo cual se arregla al tener muchos pasos del gradiente en cada época al tomar batch de datos muy pequeños en comparación al total de data disponible).\n",
    "\n",
    "A continuación se muestra los resultados al entrenar la CNN en 90 épocas más para sumar un total de 100 épocas y se puede observar que los resultados empeoran considerablemente con un claro overfitting de la red al disminuir considerablemente, a casi cero, la loss en el conjunto de train y aumentando en el conjunto de validación, lo cual también se observa con un accuracy casi de 1 en train mientras que en valid este valor disminuye, ajustando la red sólo a dichos datos de entrenamiento por realizar un entrenamiento muy largo en muchas épocas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sopjd0Z2luKX"
   },
   "source": [
    "### Entrenando en 100 épocas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "QG8jBtl71L2o",
    "outputId": "3727be92-6b62-439f-b5ab-c49bc7743e88"
   },
   "outputs": [],
   "source": [
    "history_CNN_100_epochs = modelCNN.fit(\n",
    "    X_train,\n",
    "    y_train, \n",
    "    batch_size = batch,\n",
    "    verbose = 1,\n",
    "    epochs = 90,\n",
    "    validation_data = (X_valid, y_valid)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G2uiveaL1YkB"
   },
   "outputs": [],
   "source": [
    "score_100_epochs = modelCNN.evaluate(X_test, y_test, verbose=0)\n",
    "loss_test_CNN_mean_100_epochs = score[0]\n",
    "acc_test_CNN_mean_100_epochs = score[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 470
    },
    "colab_type": "code",
    "id": "pJsHfO6-2Czb",
    "outputId": "1c3b3c0b-fef9-4415-eeb7-88cfb0e514c9"
   },
   "outputs": [],
   "source": [
    "visualize_results_CNN(history_CNN_100_epochs.history['loss'], history_CNN_100_epochs.history['val_loss'], history_CNN_100_epochs.history['accuracy'], \n",
    "                      history_CNN_100_epochs.history['val_accuracy'], loss_test_CNN_mean_100_epochs, acc_test_CNN_mean_100_epochs)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Tarea2 MLBI - José Ignacio Ortega Labra.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
